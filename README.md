## Explainability in Encrypted Neural Networks

Investigating how to provide insights into model decisions in FHE settings. This could involve creating mathematical frameworks that allow for the extraction of interpretable features from encrypted models, 
enhancing trust in secure machine learning applications. The notebook `09_Neural_network_MNIST.ipynb` uses SHAP to do so using IBM-FHE toolkit. Way to run it is given in the file `notes.txt` Another FHE example is given in 
`fhe_neuralnetwork_mnist.py` file.
