{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71cefe17",
   "metadata": {},
   "source": [
    "## Inference under FHE for the MNIST Dataset using helayers\n",
    "\n",
    "In this demo, we'll deal with a classification problem for the MNIST dataset [1], trying to correctly classify a batch of samples using a neural network model that will be created and trained using the Keras library (with architecture similar to reference [2]).\n",
    "First, we'll build a plain neural network for the MNIST model. Then, we'll encrypt the trained network and run inference over it using FHE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdf88e1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-23 08:50:31.613263: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-23 08:50:31.657867: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-23 08:50:31.968945: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-23 08:50:31.971070: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-23 08:50:33.268896: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misc. initializations\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "##### For reproducibility\n",
    "seed_value= 1\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed_value)\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow.keras import utils, losses\n",
    "import numpy as np\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, Activation\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import h5py\n",
    "\n",
    "# import activations\n",
    "import sys\n",
    "sys.path.append(os.path.join('.', 'data_gen'))\n",
    "from activations import SquareActivation\n",
    "\n",
    "PATH = os.path.join('data', 'net_mnist')\n",
    "if not os.path.exists(PATH):\n",
    "    os.makedirs(PATH)\n",
    "\n",
    "batch_size = 500\n",
    "epochs = 10\n",
    "print(\"Misc. initializations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5bb3b0",
   "metadata": {},
   "source": [
    "### Load and Preprocess the MNIST Dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c243f7f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data ready\n",
      "Added padding. New shape:  (60000, 29, 29, 1)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('data ready')\n",
    "\n",
    "# Image padding\n",
    "x_train = np.pad(x_train, ((0, 0), (0, 1), (0, 1), (0, 0)))\n",
    "x_test = np.pad(x_test, ((0, 0), (0, 1), (0, 1), (0, 0)))\n",
    "print('Added padding. New shape: ',x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30c161e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation and test data ready\n",
      "input shape: (29, 29, 1)\n"
     ]
    }
   ],
   "source": [
    "# Create validation data\n",
    "testSize=16\n",
    "x_val = x_test[:-testSize]\n",
    "x_test = x_test[-testSize:]\n",
    "y_val = y_test[:-testSize]\n",
    "y_test = y_test[-testSize:]\n",
    "print('Validation and test data ready')\n",
    "\n",
    "# Convert class vector to binary class matrices\n",
    "num_classes = 10\n",
    "y_train = utils.to_categorical(y_train, num_classes)\n",
    "y_test = utils.to_categorical(y_test, num_classes)\n",
    "y_val = utils.to_categorical(y_val, num_classes)\n",
    "\n",
    "input_shape = x_train[0].shape\n",
    "print(f'input shape: {input_shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5284f27b",
   "metadata": {},
   "source": [
    "### Save Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "969954aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving x_test of shape (16, 29, 29, 1) in data/net_mnist/x_test.h5\n",
      "Saving x_train of shape (60000, 29, 29, 1) in data/net_mnist/x_train.h5\n",
      "Saving x_val of shape (9984, 29, 29, 1) in data/net_mnist/x_val.h5\n"
     ]
    }
   ],
   "source": [
    "def save_data_set(x, y, data_type, s=''):\n",
    "    fname=os.path.join(PATH, f'x_{data_type}{s}.h5')\n",
    "    print(\"Saving x_{} of shape {} in {}\".format(data_type, x.shape,fname))\n",
    "    xf = h5py.File(fname, 'w')\n",
    "    xf.create_dataset('x_{}'.format(data_type), data=x)\n",
    "    xf.close()\n",
    "\n",
    "    yf = h5py.File(os.path.join(PATH, f'y_{data_type}{s}.h5'), 'w')\n",
    "    yf.create_dataset(f'y_{data_type}', data=y)\n",
    "    yf.close()\n",
    "\n",
    "save_data_set(x_test, y_test, data_type='test')\n",
    "save_data_set(x_train, y_train, data_type='train')\n",
    "save_data_set(x_val, y_val, data_type='val')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8a6ed1",
   "metadata": {},
   "source": [
    "### Build a Plain Neural Network for the MNIST Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27228c56",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 13, 13, 5)         130       \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 845)               0         \n",
      "                                                                 \n",
      " square_activation (SquareA  (None, 845)               0         \n",
      " ctivation)                                                      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100)               84600     \n",
      "                                                                 \n",
      " square_activation_1 (Squar  (None, 100)               0         \n",
      " eActivation)                                                    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 85740 (334.92 KB)\n",
      "Trainable params: 85740 (334.92 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(filters=5, kernel_size=5, strides=2, padding='valid',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Flatten())\n",
    "model.add(SquareActivation())\n",
    "model.add(Dense(100))\n",
    "model.add(SquareActivation())\n",
    "model.add(Dense(num_classes))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3829f0",
   "metadata": {},
   "source": [
    "### Train the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "609e5220",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "120/120 - 2s - loss: 0.3931 - accuracy: 0.8425 - val_loss: 0.2056 - val_accuracy: 0.9550 - 2s/epoch - 13ms/step\n",
      "Epoch 2/10\n",
      "120/120 - 1s - loss: 0.1774 - accuracy: 0.9594 - val_loss: 0.1477 - val_accuracy: 0.9685 - 925ms/epoch - 8ms/step\n",
      "Epoch 3/10\n",
      "120/120 - 1s - loss: 0.1390 - accuracy: 0.9701 - val_loss: 0.1267 - val_accuracy: 0.9749 - 861ms/epoch - 7ms/step\n",
      "Epoch 4/10\n",
      "120/120 - 1s - loss: 0.1201 - accuracy: 0.9754 - val_loss: 0.1130 - val_accuracy: 0.9775 - 880ms/epoch - 7ms/step\n",
      "Epoch 5/10\n",
      "120/120 - 1s - loss: 0.1083 - accuracy: 0.9790 - val_loss: 0.1049 - val_accuracy: 0.9793 - 895ms/epoch - 7ms/step\n",
      "Epoch 6/10\n",
      "120/120 - 1s - loss: 0.1000 - accuracy: 0.9810 - val_loss: 0.0983 - val_accuracy: 0.9807 - 871ms/epoch - 7ms/step\n",
      "Epoch 7/10\n",
      "120/120 - 1s - loss: 0.0937 - accuracy: 0.9833 - val_loss: 0.0942 - val_accuracy: 0.9819 - 926ms/epoch - 8ms/step\n",
      "Epoch 8/10\n",
      "120/120 - 1s - loss: 0.0890 - accuracy: 0.9842 - val_loss: 0.0914 - val_accuracy: 0.9819 - 926ms/epoch - 8ms/step\n",
      "Epoch 9/10\n",
      "120/120 - 1s - loss: 0.0853 - accuracy: 0.9853 - val_loss: 0.0900 - val_accuracy: 0.9818 - 912ms/epoch - 8ms/step\n",
      "Epoch 10/10\n",
      "120/120 - 1s - loss: 0.0821 - accuracy: 0.9862 - val_loss: 0.0864 - val_accuracy: 0.9833 - 1s/epoch - 8ms/step\n",
      "Test loss: 0.044\n",
      "Test accuracy: 100.000%\n"
     ]
    }
   ],
   "source": [
    "def sum_squared_error(y_true, y_pred):\n",
    "    return K.sum(K.square(y_pred - y_true), axis=-1)\n",
    "\n",
    "model.compile(loss=sum_squared_error,\n",
    "                  optimizer='Adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              verbose=2,\n",
    "              validation_data=(x_val, y_val),\n",
    "              shuffle=True,\n",
    "              )\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print(f'Test loss: { score[0]:.3f}')\n",
    "print(f'Test accuracy: {score[1] * 100:.3f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2477a1b2",
   "metadata": {},
   "source": [
    "### Report the Confusion Matrix of the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbf299e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 51ms/step\n",
      "[[1 0 0 0 0 0 0 0 0 0]\n",
      " [0 2 0 0 0 0 0 0 0 0]\n",
      " [0 0 2 0 0 0 0 0 0 0]\n",
      " [0 0 0 2 0 0 0 0 0 0]\n",
      " [0 0 0 0 2 0 0 0 0 0]\n",
      " [0 0 0 0 0 2 0 0 0 0]\n",
      " [0 0 0 0 0 0 2 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "y_pred_vals = model.predict(x_test)\n",
    "y_pred = np.argmax(y_pred_vals, axis=1)\n",
    "y_test = np.argmax(y_test, axis=1)\n",
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae15beff-2056-4bcd-a619-e514d8ebf518",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312/312 [==============================] - 1s 2ms/step\n",
      "[[ 971    1    4    0    1    0    0    1    1    0]\n",
      " [   0 1124    3    1    1    1    1    1    1    0]\n",
      " [   3    0 1017    0    0    0    1    5    3    1]\n",
      " [   0    0    5  991    0    4    0    4    4    0]\n",
      " [   1    0    0    0  964    0    4    1    2    8]\n",
      " [   2    1    0    5    0  877    3    1    1    0]\n",
      " [   5    3    0    1    1    4  939    1    2    0]\n",
      " [   0   10   11    0    1    1    0 1000    1    3]\n",
      " [   2    0    2    3    0    2    0    3  959    2]\n",
      " [   2    2    1    3   10    3    0    7    5  975]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "y_pred_vals = model.predict(x_val)\n",
    "y_pred_max = np.argmax(y_pred_vals, axis=1)\n",
    "y_val_max = np.argmax(y_val, axis=1)\n",
    "cm = metrics.confusion_matrix(y_val_max, y_pred_max)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "737c7f55-904d-49f9-804a-0aec1f9653a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 19ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "982467bca75544d095e28aafafb84ccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 14ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x454.545 with 23 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAP explainability completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import shap\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---- Dimensionality Reduction with Pooling ----\n",
    "def reduce_with_pooling(images, pool_size=2):\n",
    "    \"\"\"\n",
    "    Reduce the feature space using average pooling to downsample images.\n",
    "    \"\"\"\n",
    "    reduced_images = tf.nn.avg_pool(\n",
    "        images,\n",
    "        ksize=(1, pool_size, pool_size, 1),\n",
    "        strides=(1, pool_size, pool_size, 1),\n",
    "        padding='VALID'\n",
    "    )\n",
    "    return reduced_images.numpy()\n",
    "\n",
    "# Apply pooling to reduce dimensions for both background and test samples\n",
    "pool_size = 2  # Pooling factor\n",
    "background_reduced = reduce_with_pooling(x_train[:10])  # First 10 training images as background\n",
    "x_test_reduced = reduce_with_pooling(x_test[:2])  # First 2 test samples for explanation\n",
    "\n",
    "# Flatten the pooled images for SHAP compatibility\n",
    "background_flattened = background_reduced.reshape((background_reduced.shape[0], -1))\n",
    "x_test_flattened = x_test_reduced.reshape((x_test_reduced.shape[0], -1))\n",
    "\n",
    "# ---- Define Prediction Function ----\n",
    "def predict_fn(data):\n",
    "    \"\"\"\n",
    "    Map reduced representations back to the full-size input for predictions.\n",
    "    \"\"\"\n",
    "    full_images = tf.image.resize(tf.convert_to_tensor(data.reshape((-1, *background_reduced.shape[1:]))), (29, 29))\n",
    "    return model.predict(full_images.numpy())\n",
    "\n",
    "# ---- Initialize SHAP KernelExplainer ----\n",
    "explainer = shap.KernelExplainer(predict_fn, background_flattened)\n",
    "\n",
    "# ---- Compute SHAP Values ----\n",
    "shap_values = explainer.shap_values(x_test_flattened, nsamples=100)  # Limit to 100 perturbations\n",
    "\n",
    "# ---- Adjust SHAP Visualization ----\n",
    "# Reshape SHAP values to match the original image dimensions (if needed)\n",
    "shap_values_reshaped = [value.reshape((-1, *background_reduced.shape[1:])) for value in shap_values]\n",
    "\n",
    "# Visualize SHAP explanations with properly structured data\n",
    "shap.image_plot(shap_values_reshaped, x_test[:2])  # Match test samples with SHAP values\n",
    "\n",
    "# Save SHAP values if needed\n",
    "np.save('shap_values.npy', shap_values)\n",
    "\n",
    "print(\"SHAP explainability completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd6ade2-b5ad-4ed3-8a67-d9f0ee1e86be",
   "metadata": {},
   "source": [
    "The presence of 10 SHAP plots (or columns) per sample corresponds to the 10 output classes of the model, since your model is trained on the MNIST dataset, which has 10 possible digit classes (0 through 9).\n",
    "\n",
    "Here's what each column represents:\n",
    "\n",
    "1. SHAP Values for Each Class\n",
    "Each column shows the SHAP explanation for how the features (pixels) contribute to the probability of the corresponding digit class. For example:\n",
    "\n",
    "The first column explains the contribution of the image's pixels to the model's probability for the digit \"0.\"\n",
    "\n",
    "The second column does the same for the probability of the digit \"1.\"\n",
    "\n",
    "This continues up to the last column, which explains the contribution of the pixels to the probability of the digit \"9.\"\n",
    "\n",
    "2. Interpretation for Each Column\n",
    "Red Areas: These pixels increase the probability for the respective digit class.\n",
    "\n",
    "Blue Areas: These pixels decrease the probability for the respective digit class.\n",
    "\n",
    "The magnitude of the red or blue colors indicates the strength of the contribution (positive or negative) to the specific class.\n",
    "\n",
    "3. Why Are There 10 Columns per Sample?\n",
    "The model outputs a probability distribution over all 10 classes for each input image. The SHAP framework calculates feature contributions for each class independently, even though the predicted class might be just one of them.\n",
    "\n",
    "For example:\n",
    "\n",
    "If the sample is an image of the digit \"1,\" you might see:\n",
    "\n",
    "Strong red contributions in the second column (digit \"1\"), as the relevant pixels reinforce this prediction.\n",
    "\n",
    "Predominantly blue contributions in other columns (e.g., \"0,\" \"2,\" etc.), showing how the same pixels decrease the probability of those other classes.\n",
    "\n",
    "Takeaway\n",
    "The 10 plots provide a holistic view of how the model interprets an image across all possible classes. This helps you verify whether the model is confidently focusing on relevant features for the correct class, while also checking if it mistakenly assigns probabilities to irrelevant classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e4b519",
   "metadata": {},
   "source": [
    "### Serialize Model and Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82393d1a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model_json = model.to_json()\n",
    "with open(os.path.join(PATH, 'model.json'), \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(os.path.join(PATH, 'model.h5'))\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9211fd0b",
   "metadata": {},
   "source": [
    "We are all done training the plain network. Next we will encrypt the network and run inference over it using FHE. Let's start with some initializations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aee4130e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mWarning: total available memory is 7.481624603271484 GB the minimum required memory is 8 GB\n",
      "\u001b[0;0mMisc. initializations\n"
     ]
    }
   ],
   "source": [
    "import pyhelayers\n",
    "import utils\n",
    "\n",
    "utils.verify_memory()\n",
    "\n",
    "print('Misc. initializations')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd942b0e",
   "metadata": {},
   "source": [
    "The following is a general outline of the next steps.\n",
    "\n",
    "We encode and encrypt a neural network model, using the files that were created and saved before. An automated optimizer, that occurs during the call to encode_encrypt, will examine our network and will determine various configuration details that will allow running inference under encryption efficiently.\n",
    "\n",
    "Next, we will demonstrate how we can encrypt data, run inference on our encrypted network, and compare the results against the expected labels.\n",
    "Now let's dive in . . ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "40e11341",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "he_run_req = pyhelayers.HeRunRequirements()\n",
    "he_run_req.set_he_context_options([pyhelayers.DefaultContext()])\n",
    "he_run_req.optimize_for_batch_size(16)\n",
    "\n",
    "nn = pyhelayers.NeuralNet()\n",
    "nn.encode_encrypt([os.path.join(PATH, \"model.json\"), os.path.join(PATH, \"model.h5\")], he_run_req)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edbc4be",
   "metadata": {},
   "source": [
    "The encode_encrypt method also initialized an HeContext object containing the keys. We obtain it now from the model so we can encrypt the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1730768d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "context = nn.get_created_he_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91afd062",
   "metadata": {},
   "source": [
    "We will now load real samples of the MNIST dataset to classify. We will load the samples and the corresponding true labels from HDF5 files. We will also extract the first batch of samples and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3cb16a44",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch of size 500 loaded\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(os.path.join(PATH, \"x_test.h5\")) as f:\n",
    "    x_test = np.array(f[\"x_test\"])\n",
    "with h5py.File(os.path.join(PATH, \"y_test.h5\")) as f:\n",
    "    y_test = np.array(f[\"y_test\"])\n",
    "    \n",
    "plain_samples, labels = utils.extract_batch(x_test, y_test, batch_size, 0)\n",
    "\n",
    "print('Batch of size',batch_size,'loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a4eee5",
   "metadata": {},
   "source": [
    "To populate the input, we need to encode and then encrypt the values of the plain input under HE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f8a882b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data encrypted\n"
     ]
    }
   ],
   "source": [
    "model_io_encoder = pyhelayers.ModelIoEncoder(nn)\n",
    "samples = pyhelayers.EncryptedData(context)\n",
    "model_io_encoder.encode_encrypt(samples, [plain_samples])\n",
    "print('Test data encrypted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59df1642",
   "metadata": {},
   "source": [
    "We now go ahead with the inference itself. We run the encrypted input through the encrypted NN to obtain encrypted results. This computation does not use the secret key and acts on completely encrypted values. Running the inference is done using the \"predict\" method of the NN, that receives the destination 3D structure to put the result of the computation in, and the input for the inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "00f4d6ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration of predict: 0.906 (s)\n",
      "Duration of predict per sample: 0.002 (s)\n"
     ]
    }
   ],
   "source": [
    "utils.start_timer()\n",
    "\n",
    "predictions = pyhelayers.EncryptedData(context)\n",
    "nn.predict(predictions, samples)\n",
    "\n",
    "duration=utils.end_timer('predict')\n",
    "utils.report_duration('predict per sample',duration/batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ecf2a9",
   "metadata": {},
   "source": [
    "In order to assess the results of the computation, we first need to decrypt them. This is done by an IO processor that has the secret key and is capable of decrypting the ciphertext and decoding it into plaintext version of the HE computation result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a17629ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions [[ 0.01508655  0.9556766  -0.03254654 -0.0498449  -0.02826862 -0.03830043\n",
      "   0.03392333  0.12958999  0.03242511 -0.04932714]\n",
      " [-0.02009235  0.07079617  0.81363009  0.08180354 -0.07353853 -0.06460142\n",
      "   0.075831    0.20277292  0.0717024  -0.12123238]\n",
      " [-0.02696569 -0.01127931 -0.02921735  0.94552477  0.03677153  0.01486113\n",
      "   0.02564035 -0.04214175  0.01752971  0.06823581]\n",
      " [-0.0211434  -0.03980317  0.00716084  0.00199379  1.12982181 -0.04549023\n",
      "   0.00759635 -0.02203351 -0.03035469  0.00329346]\n",
      " [-0.0122534   0.03895842  0.01073258 -0.09541493 -0.01397666  1.0175656\n",
      "   0.04253887  0.02883085 -0.02768353 -0.05722819]\n",
      " [-0.02556069 -0.01277226  0.02841343 -0.03513888 -0.01457178  0.03727296\n",
      "   1.03760964 -0.01876368  0.01511617 -0.00418618]\n",
      " [-0.03429304 -0.00731335 -0.02504475 -0.00593381 -0.01856585 -0.009705\n",
      "   0.0330103   1.05247671  0.01809032  0.04164397]\n",
      " [ 0.02558459  0.07087585  0.00838081 -0.06852435  0.20419284  0.01208903\n",
      "  -0.00149254 -0.11654867  0.8952453  -0.1003163 ]\n",
      " [-0.03799306 -0.00829021  0.02098139 -0.02585925  0.20561075 -0.0333165\n",
      "  -0.02913166 -0.07604563 -0.01528036  1.03264078]\n",
      " [ 0.64635414 -0.02781183  0.08774725  0.00733315 -0.07797991  0.0418152\n",
      "  -0.04233747  0.05082487  0.14509738  0.10027853]\n",
      " [-0.00219596  0.97951897  0.03012648 -0.0252521   0.00158108 -0.0079007\n",
      "   0.01698326 -0.00149296 -0.01536589  0.00419964]\n",
      " [ 0.01476943  0.02527408  1.2031678  -0.07180345  0.01017868  0.01837542\n",
      "  -0.01646701 -0.02357389 -0.020649   -0.05601906]\n",
      " [ 0.0113301   0.00588593  0.01346756  0.95810269 -0.04545638 -0.03472983\n",
      "   0.00422219 -0.03102967  0.007718    0.04374086]\n",
      " [ 0.07474096 -0.02041337  0.0662263  -0.05004852  1.01028113 -0.07426049\n",
      "   0.01286392  0.04967729 -0.05636167 -0.01202069]\n",
      " [-0.05687768 -0.06975618  0.03976407 -0.03965443 -0.11702268  1.12800956\n",
      "   0.01440642 -0.01655638  0.07265306 -0.01522957]\n",
      " [ 0.05198497 -0.01517716  0.01938664  0.06833943  0.0080213   0.00709296\n",
      "   0.83953692 -0.02542403 -0.0097324  -0.01009746]]\n"
     ]
    }
   ],
   "source": [
    "plain_predictions = model_io_encoder.decrypt_decode_output(predictions)\n",
    "print('predictions',plain_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d884b6a5",
   "metadata": {},
   "source": [
    "Now we compare the results against the expected labels and compute the confusion matrix and the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3258d071",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "[[1 0 0 0 0 0 0 0 0 0]\n",
      " [0 2 0 0 0 0 0 0 0 0]\n",
      " [0 0 2 0 0 0 0 0 0 0]\n",
      " [0 0 0 2 0 0 0 0 0 0]\n",
      " [0 0 0 0 2 0 0 0 0 0]\n",
      " [0 0 0 0 0 2 0 0 0 0]\n",
      " [0 0 0 0 0 0 2 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.assess_results(labels, plain_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ac7f3b3a-5f45-4b78-b2a5-fc979b871b3d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mWarning: total available memory is 7.481624603271484 GB the minimum required memory is 8 GB\n",
      "\u001b[0;0mInitializing homomorphic encryption for SHAP values...\n",
      "Shape of combined SHAP values before resizing: (2, 14, 14, 1)\n",
      "Shape of combined SHAP values after resizing: (2, 29, 29, 1)\n",
      "SHAP values encrypted successfully.\n",
      "Decrypted network predictions from encrypted SHAP values: [[ 0.03924931  0.30495556  0.11677175  0.08060453  0.02740902  0.40723772\n",
      "   0.03024192  0.14935137 -0.11697674 -0.01308532]\n",
      " [ 0.03958081  0.30490711  0.11654149  0.08044238  0.02735919  0.40732359\n",
      "   0.03000222  0.14937533 -0.11687706 -0.01298974]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pyhelayers\n",
    "import utils          # from your existing FHE setup\n",
    "import tensorflow as tf  # for resizing\n",
    "\n",
    "# Verify memory and initialize FHE context\n",
    "utils.verify_memory()\n",
    "print('Initializing homomorphic encryption for SHAP values...')\n",
    "\n",
    "he_run_req = pyhelayers.HeRunRequirements()\n",
    "he_run_req.set_he_context_options([pyhelayers.DefaultContext()])\n",
    "he_run_req.optimize_for_batch_size(16)\n",
    "\n",
    "# Initialize and encrypt the neural network model\n",
    "nn = pyhelayers.NeuralNet()\n",
    "nn.encode_encrypt([os.path.join(PATH, \"model.json\"), os.path.join(PATH, \"model.h5\")], he_run_req)\n",
    "context = nn.get_created_he_context()\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Assume you have computed your SHAP values already.\n",
    "# shap_values_reshaped is a list of 10 arrays (one per class) where each array\n",
    "# has shape (n_samples, h, w, 1). In your case the intermediate shape is (n_samples, 14, 14, 1).\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# Combine the 10 outputs into one by averaging over the class axis.\n",
    "combined_shap_values = np.mean(np.array(shap_values_reshaped), axis=0)\n",
    "print(\"Shape of combined SHAP values before resizing:\", combined_shap_values.shape)\n",
    "# For example, (n_samples, 14, 14, 1)\n",
    "\n",
    "# Your FHE model expects inputs of shape (-1, 29, 29, 1) so resize if necessary:\n",
    "expected_shape = (29, 29)\n",
    "if combined_shap_values.shape[1:3] != expected_shape:\n",
    "    # Resize the images to match expected dimensions (using bilinear interpolation)\n",
    "    combined_shap_values = tf.image.resize(combined_shap_values, expected_shape,\n",
    "                                           method=tf.image.ResizeMethod.BILINEAR).numpy()\n",
    "\n",
    "print(\"Shape of combined SHAP values after resizing:\", combined_shap_values.shape)\n",
    "# Now should be (n_samples, 29, 29, 1)\n",
    "\n",
    "# Prepare the EncryptedData object and the Model I/O encoder.\n",
    "shap_encrypted = pyhelayers.EncryptedData(context)\n",
    "shap_io_encoder = pyhelayers.ModelIoEncoder(nn)\n",
    "\n",
    "# Encrypt the combined SHAP values.\n",
    "# The encoder expects a list with one numpy array matching the networks input shape.\n",
    "shap_io_encoder.encode_encrypt(shap_encrypted, [combined_shap_values])\n",
    "print('SHAP values encrypted successfully.')\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Verification: Instead of decrypting the raw encrypted input, \n",
    "# perform a forward pass through the encrypted neural network.\n",
    "# This simulates the normal FHE workflow.\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# Create an EncryptedData object to hold the network's output.\n",
    "encrypted_output = pyhelayers.EncryptedData(context)\n",
    "\n",
    "# Pass the encrypted SHAP values through the network's prediction routine.\n",
    "nn.predict(encrypted_output, shap_encrypted)\n",
    "\n",
    "# Decrypt and decode the network's output.\n",
    "decrypted_output = shap_io_encoder.decrypt_decode_output(encrypted_output)\n",
    "print('Decrypted network predictions from encrypted SHAP values:', decrypted_output)\n",
    "\n",
    "# Note:\n",
    "# These predictions correspond to the neural network's output for the given SHAP values.\n",
    "# They may not be interpretable as \"SHAP values\" since the network's forward path was applied.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da22a70e-8d78-4b72-8631-06f7d5b102e4",
   "metadata": {},
   "source": [
    "In our FHE framework the method decrypt_decode_output() is designed to decrypt the output of the neural networks forward pass, not an encrypted input. (When you encrypt your training or test data you later call the networks predict routine and then decrypt its output.)\n",
    "\n",
    "To verify (i.e. to do a roundtrip) that your encryption works correctly, you should run a forward pass on your encrypted SHAP values through your (FHEencrypted) neural network, then decrypt the networks output. (Notethe decrypted output will be the networks prediction when given SHAP values as input; it wont be the original SHAP values, but it will verify that encryption and evaluation are working.)\n",
    "\n",
    "Below is the modified code for verification that replaces the direct decryption of the input with a call to the networks predict method before decryption:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "652467e7-2349-4dc0-98c6-2dd53548e5b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mWarning: total available memory is 7.481624603271484 GB the minimum required memory is 8 GB\n",
      "\u001b[0;0mInitializing homomorphic encryption for SHAP values...\n",
      "Shape of combined SHAP values before resizing: (2, 14, 14, 1)\n",
      "Shape of combined SHAP values after resizing: (2, 29, 29, 1)\n",
      "SHAP values encrypted successfully.\n",
      "Running encrypted inference on the SHAP values...\n",
      "Decrypted network predictions from encrypted SHAP values:\n",
      "[[ 0.03924931  0.30495556  0.11677175  0.08060453  0.02740902  0.40723772\n",
      "   0.03024192  0.14935137 -0.11697674 -0.01308532]\n",
      " [ 0.03958081  0.30490711  0.11654149  0.08044238  0.02735919  0.40732359\n",
      "   0.03000222  0.14937533 -0.11687706 -0.01298974]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pyhelayers\n",
    "import utils           # Your helper module from the FHE setup\n",
    "import tensorflow as tf  # Used for resizing\n",
    "\n",
    "# Assume PATH is set and points to the folder with model.json and model.h5.\n",
    "# Also assume shap_values_reshaped is already computed: a list of 10 arrays (one per class)\n",
    "# For instance: shap_values_reshaped = [shap_class0, shap_class1, ..., shap_class9]\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 1. Initialize and Encrypt Neural Network Model (FHE Setup)\n",
    "# ------------------------------------------------------------------------------\n",
    "utils.verify_memory()\n",
    "print(\"Initializing homomorphic encryption for SHAP values...\")\n",
    "\n",
    "he_run_req = pyhelayers.HeRunRequirements()\n",
    "he_run_req.set_he_context_options([pyhelayers.DefaultContext()])\n",
    "he_run_req.optimize_for_batch_size(16)\n",
    "\n",
    "nn = pyhelayers.NeuralNet()\n",
    "nn.encode_encrypt([os.path.join(PATH, \"model.json\"), os.path.join(PATH, \"model.h5\")], he_run_req)\n",
    "context = nn.get_created_he_context()\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 2. Process SHAP Values to Match Network Input Requirements\n",
    "# ------------------------------------------------------------------------------\n",
    "# Combine the 10 per-class SHAP outputs by averaging over the class axis.\n",
    "combined_shap_values = np.mean(np.array(shap_values_reshaped), axis=0)\n",
    "print(\"Shape of combined SHAP values before resizing:\", combined_shap_values.shape)\n",
    "# (For example, this may print: (n_samples, 14, 14, 1))\n",
    "\n",
    "# The network expects its input to be shaped as (-1, 29, 29, 1).\n",
    "expected_spatial_dims = (29, 29)\n",
    "if combined_shap_values.shape[1:3] != expected_spatial_dims:\n",
    "    combined_shap_values = tf.image.resize(combined_shap_values, expected_spatial_dims,\n",
    "                                           method=tf.image.ResizeMethod.BILINEAR).numpy()\n",
    "print(\"Shape of combined SHAP values after resizing:\", combined_shap_values.shape)\n",
    "# Now, should be: (n_samples, 29, 29, 1)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 3. Encrypt the Combined SHAP Values \n",
    "# ------------------------------------------------------------------------------\n",
    "# Create an EncryptedData object and use the ModelIoEncoder to encrypt.\n",
    "shap_encrypted = pyhelayers.EncryptedData(context)\n",
    "shap_io_encoder = pyhelayers.ModelIoEncoder(nn)\n",
    "\n",
    "# The encoder expects a list with exactly one NumPy array (matching the network input)\n",
    "shap_io_encoder.encode_encrypt(shap_encrypted, [combined_shap_values])\n",
    "print(\"SHAP values encrypted successfully.\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 4. Verification: Run Encrypted Inference and Decrypt the Output\n",
    "# ------------------------------------------------------------------------------\n",
    "# Instead of decrypting the raw input, perform a forward encrypted inference.\n",
    "encrypted_output = pyhelayers.EncryptedData(context)\n",
    "\n",
    "print(\"Running encrypted inference on the SHAP values...\")\n",
    "nn.predict(encrypted_output, shap_encrypted)\n",
    "\n",
    "# Decrypt and decode the network's predictions.\n",
    "decrypted_output = shap_io_encoder.decrypt_decode_output(encrypted_output)\n",
    "print(\"Decrypted network predictions from encrypted SHAP values:\")\n",
    "print(decrypted_output)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Note:\n",
    "# The decrypted_output gives you the network's output (predictions) when provided with\n",
    "# the encrypted SHAP values as input. This round-trip verifies that the encryption,\n",
    "# network evaluation, and decryption steps are functioning as expected.\n",
    "# ------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e29731-24e1-4eee-afba-312ecf2bc432",
   "metadata": {},
   "source": [
    "\n",
    "Plaintext Predictions: These are obtained by running your original Keras model on the combined SHAP values in cleartext. They represent how the model \"sees\" the explanation information from the SHAP values.\n",
    "\n",
    "FHE Predictions: These are obtained by encrypting the same combined SHAP values, running them through your FHE-encrypted model, and then decrypting the output. The FHE process is designed to yield (within a small numerical margin due to approximations) the same output as the plaintext inference.\n",
    "\n",
    "Comparing Predictions: By comparing the two sets of predictions (e.g., via an absolute difference), you can verify that the FHE pipeline preserves the essential explanatory information contained in the SHAP values. In an ideal scenario, the predictions should be nearly identical (perhaps differing only in small numerical precision or rounding).\n",
    "\n",
    "Regarding the Explanation: Ultimately, if the FHE and plaintext predictions agree, then you have confidence that applying FHE to your SHAP values retains the same information. You could then use the decrypted FHE inference as an indicator of how the SHAP values support a given class. For instance, iffor a particular samplethe highest score in both plaintext and FHE predictions corresponds to the same class, you can assert that the FHE process correctly preserves the models explanation properties.\n",
    "\n",
    "This code, and its comparison of plaintext versus FHE predictions, fulfills the objective of checking that the encrypted SHAP values preserve the same information as the original SHAP values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "527ab59f-486d-4d71-96c2-726b15e2daaa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of combined SHAP values before resizing: (2, 14, 14, 1)\n",
      "Shape of combined SHAP values after resizing: (2, 29, 29, 1)\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "Plaintext predictions from combined SHAP values:\n",
      "[[ 0.03924934  0.3049555   0.11677174  0.08060454  0.027409    0.40723768\n",
      "   0.03024194  0.14935136 -0.11697674 -0.01308529]\n",
      " [ 0.03958081  0.30490708  0.1165415   0.08044241  0.0273592   0.40732357\n",
      "   0.0300022   0.14937532 -0.11687708 -0.01298973]]\n",
      "\u001b[1;31mWarning: total available memory is 7.481624603271484 GB the minimum required memory is 8 GB\n",
      "\u001b[0;0mInitializing homomorphic encryption for SHAP values...\n",
      "SHAP values encrypted successfully.\n",
      "Running encrypted inference on the combined SHAP values...\n",
      "FHE predictions (decrypted) from combined SHAP values:\n",
      "[[ 0.0392493   0.30495556  0.11677175  0.08060453  0.02740902  0.40723772\n",
      "   0.03024192  0.14935137 -0.11697674 -0.01308532]\n",
      " [ 0.03958081  0.30490711  0.1165415   0.08044239  0.02735919  0.40732359\n",
      "   0.03000222  0.14937533 -0.11687706 -0.01298974]]\n",
      "Difference between plaintext and FHE predictions:\n",
      "[[3.78212395e-08 4.63221571e-08 1.20918012e-08 1.28922294e-08\n",
      "  1.77212182e-08 3.91844437e-08 2.13295836e-08 1.35780502e-08\n",
      "  6.20907309e-09 2.95746370e-08]\n",
      " [1.77105555e-09 2.84262747e-08 9.84874489e-09 2.85424582e-08\n",
      "  6.88815010e-09 2.29201387e-08 2.53215090e-08 9.82416462e-09\n",
      "  1.80216403e-08 1.27619468e-08]]\n",
      "Maximum difference: 4.6322157110090245e-08\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pyhelayers\n",
    "import utils                # Your helper module from the FHE setup\n",
    "import tensorflow as tf     # For resizing\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 1. Load/Define your combined SHAP values (from your SHAP explainability)\n",
    "#    Here we assume shap_values_reshaped is a list of 10 arrays (one per class)\n",
    "#    with shape (n_samples, 14, 14, 1). We combine them by averaging and resize.\n",
    "# ---------------------------------------------------------------------------\n",
    "combined_shap_values = np.mean(np.array(shap_values_reshaped), axis=0)\n",
    "print(\"Shape of combined SHAP values before resizing:\", combined_shap_values.shape)\n",
    "expected_spatial_dims = (29, 29)\n",
    "if combined_shap_values.shape[1:3] != expected_spatial_dims:\n",
    "    combined_shap_values = tf.image.resize(combined_shap_values, expected_spatial_dims,\n",
    "                                           method=tf.image.ResizeMethod.BILINEAR).numpy()\n",
    "print(\"Shape of combined SHAP values after resizing:\", combined_shap_values.shape)\n",
    "# Now combined_shap_values has shape (n_samples, 29, 29, 1)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 2. Obtain Plaintext Predictions (Reference)\n",
    "# ---------------------------------------------------------------------------\n",
    "# (Assuming your original plain Keras model is available as 'model')\n",
    "plaintext_predictions = model.predict(combined_shap_values)\n",
    "print(\"Plaintext predictions from combined SHAP values:\")\n",
    "print(plaintext_predictions)\n",
    "# These predictions are what you would get if you fed the combined SHAP values to your model in the clear.\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3. Set Up the FHE Environment and Encrypt the Combined SHAP Values\n",
    "# ---------------------------------------------------------------------------\n",
    "utils.verify_memory()\n",
    "print(\"Initializing homomorphic encryption for SHAP values...\")\n",
    "\n",
    "he_run_req = pyhelayers.HeRunRequirements()\n",
    "he_run_req.set_he_context_options([pyhelayers.DefaultContext()])\n",
    "he_run_req.optimize_for_batch_size(16)\n",
    "\n",
    "# Initialize and encrypt the neural network model from FHE files.\n",
    "nn = pyhelayers.NeuralNet()\n",
    "nn.encode_encrypt([os.path.join(PATH, \"model.json\"), os.path.join(PATH, \"model.h5\")], he_run_req)\n",
    "context = nn.get_created_he_context()\n",
    "\n",
    "# Create an EncryptedData object for the input and a ModelIoEncoder.\n",
    "shap_encrypted = pyhelayers.EncryptedData(context)\n",
    "shap_io_encoder = pyhelayers.ModelIoEncoder(nn)\n",
    "\n",
    "# Encrypt the combined SHAP values. \n",
    "# Note: The encoder expects a [numpy array] that matches the network's input shape.\n",
    "shap_io_encoder.encode_encrypt(shap_encrypted, [combined_shap_values])\n",
    "print(\"SHAP values encrypted successfully.\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 4. Run Encrypted Inference and Decrypt the Prediction Output\n",
    "# ---------------------------------------------------------------------------\n",
    "encrypted_output = pyhelayers.EncryptedData(context)\n",
    "print(\"Running encrypted inference on the combined SHAP values...\")\n",
    "nn.predict(encrypted_output, shap_encrypted)\n",
    "\n",
    "# Decrypt and decode the network's output.\n",
    "fhe_predictions = shap_io_encoder.decrypt_decode_output(encrypted_output)\n",
    "print(\"FHE predictions (decrypted) from combined SHAP values:\")\n",
    "print(fhe_predictions)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 5. Compare the Results\n",
    "# ---------------------------------------------------------------------------\n",
    "difference = np.abs(plaintext_predictions - fhe_predictions)\n",
    "print(\"Difference between plaintext and FHE predictions:\")\n",
    "print(difference)\n",
    "\n",
    "# Optionally, you could compute aggregate statistics like maximum difference:\n",
    "max_diff = np.max(difference)\n",
    "print(\"Maximum difference:\", max_diff)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd1f421-5ec8-4d33-9aa0-fcb4b25831bb",
   "metadata": {},
   "source": [
    "Below is one integrated code example that meets the following objective:\n",
    "\n",
    "Build a SHAP explainer on plain training data (background) using your plain x_train.\n",
    "\n",
    "Define two prediction functions for your neural network:\n",
    "\n",
    "A plain prediction function (using your unencrypted model).\n",
    "\n",
    "An FHE prediction function that encrypts incoming data using pyhelayers (with your previously built FHE model), runs encrypted inference, then decrypts the result.\n",
    "\n",
    "Use the same SHAP explainer framework (here we use KernelExplainer for illustration) with both prediction functions on a (small) set of x_test samples.\n",
    "\n",
    "Compare the raw SHAP values obtained in the clear (plain) with those computed via the FHEencrypted pathway (which you decrypt to recover the SHAP values).\n",
    "\n",
    "In effect, you verify that applying FHE to the explanation (via the prediction function inside SHAP) preserves the information encoded in the SHAP values.\n",
    "\n",
    "Below is the complete code. (Adjust paths, nsamples, and sample counts as needed.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725f55cb-adbd-4730-9524-1502701a38d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mWarning: total available memory is 7.481624603271484 GB the minimum required memory is 8 GB\n",
      "\u001b[0;0mInitializing FHE for prediction...\n",
      "32/32 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 1000 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import shap\n",
    "import pyhelayers\n",
    "import utils  # Your helper module for FHE setup\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. Data Preparation\n",
    "# -----------------------------------------------------------------------------\n",
    "# For KernelExplainer to work reliably, the number of background samples must\n",
    "# exceed the number of features. Since each image (29,29,1) flattens to 841 features,\n",
    "# we choose a background size of 1000 samples (or any number >>841).\n",
    "background = x_train[:1000]       # shape: (1000, 29, 29, 1)\n",
    "x_test_small = x_test[:2]         # shape: (2, 29, 29, 1)\n",
    "\n",
    "# Flatten images to 1D (KernelExplainer requires 1D or 2D input).\n",
    "background_flat = background.reshape((background.shape[0], -1))  # shape: (1000, 841)\n",
    "x_test_small_flat = x_test_small.reshape((x_test_small.shape[0], -1))  # shape: (2, 841)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2. Define Prediction Functions that work on flattened input\n",
    "# -----------------------------------------------------------------------------\n",
    "def plain_predict_flat(flat_data):\n",
    "    \"\"\"\n",
    "    Takes flattened inputs (n_samples, 841), reshapes to (-1,29,29,1),\n",
    "    and returns predictions from the plain model.\n",
    "    \"\"\"\n",
    "    reshaped = flat_data.reshape(-1, 29, 29, 1)\n",
    "    return model.predict(reshaped)\n",
    "\n",
    "# Initialize FHE components for encrypted inference\n",
    "utils.verify_memory()\n",
    "print(\"Initializing FHE for prediction...\")\n",
    "he_run_req = pyhelayers.HeRunRequirements()\n",
    "he_run_req.set_he_context_options([pyhelayers.DefaultContext()])\n",
    "he_run_req.optimize_for_batch_size(16)\n",
    "\n",
    "nn = pyhelayers.NeuralNet()\n",
    "nn.encode_encrypt([os.path.join(PATH, \"model.json\"), os.path.join(PATH, \"model.h5\")],\n",
    "                  he_run_req)\n",
    "context = nn.get_created_he_context()\n",
    "shap_io_encoder = pyhelayers.ModelIoEncoder(nn)\n",
    "\n",
    "def fhe_predict_flat(flat_data):\n",
    "    \"\"\"\n",
    "    Takes flattened input (n_samples, 841), reshapes to (-1,29,29,1),\n",
    "    encrypts it, runs FHE inference, decrypts output, and returns predictions.\n",
    "    \"\"\"\n",
    "    reshaped = flat_data.reshape(-1, 29, 29, 1)\n",
    "    # Encrypt the input.\n",
    "    encrypted_input = pyhelayers.EncryptedData(context)\n",
    "    shap_io_encoder.encode_encrypt(encrypted_input, [reshaped])\n",
    "    \n",
    "    # Perform encrypted prediction.\n",
    "    encrypted_output = pyhelayers.EncryptedData(context)\n",
    "    nn.predict(encrypted_output, encrypted_input)\n",
    "    \n",
    "    # Decrypt the output.\n",
    "    decrypted_result = shap_io_encoder.decrypt_decode_output(encrypted_output)\n",
    "    return decrypted_result\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3. Build SHAP Explainers Using the Flattened Background\n",
    "# -----------------------------------------------------------------------------\n",
    "# Setting l1_reg to a fixed constant (e.g. 0.001) avoids the default noise variance\n",
    "# estimation that causes the under-determined error when background size is small.\n",
    "l1_reg_constant = 0.001\n",
    "\n",
    "explainer_plain = shap.KernelExplainer(plain_predict_flat, background_flat, l1_reg=l1_reg_constant)\n",
    "explainer_fhe   = shap.KernelExplainer(fhe_predict_flat, background_flat,   l1_reg=l1_reg_constant)\n",
    "\n",
    "# Choose a modest number of samples for perturbations.\n",
    "nsamples = 50\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4. Compute SHAP Values for x_test_small (Flattened) for Both Methods\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"Computing plain SHAP values...\")\n",
    "plain_shap_values = explainer_plain.shap_values(x_test_small_flat, nsamples=nsamples)\n",
    "print(\"Plain SHAP values:\")\n",
    "print(plain_shap_values)\n",
    "\n",
    "print(\"Computing FHE SHAP values (via encrypted prediction)...\")\n",
    "fhe_shap_values = explainer_fhe.shap_values(x_test_small_flat, nsamples=nsamples)\n",
    "print(\"FHE SHAP values (after decryption):\")\n",
    "print(fhe_shap_values)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5. Verification: Compare the Plain vs. FHE SHAP Values\n",
    "# -----------------------------------------------------------------------------\n",
    "difference = np.abs(np.array(plain_shap_values) - np.array(fhe_shap_values))\n",
    "print(\"Difference between Plain and FHE SHAP values:\")\n",
    "print(difference)\n",
    "\n",
    "max_diff = np.max(difference)\n",
    "print(\"Maximum difference:\", max_diff)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae69f9e",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "References:\n",
    "\n",
    "<sub><sup> 1.\tLeCun, Yann and Cortes, Corinna. \"MNIST handwritten digit database.\" (2010): </sup></sub>\n",
    "\n",
    "<sub><sup> 2.\tGilad-Bachrach, R., Dowlin, N., Laine, K., Lauter, K., Naehrig, M. &amp; Wernsing, J.. (2016). CryptoNets: Applying Neural Networks to Encrypted Data with High Throughput and Accuracy. Proceedings of The 33rd International Conference on Machine Learning, in Proceedings of Machine Learning Research 48:201-210 Available from https://proceedings.mlr.press/v48/gilad-bachrach16.html.\n",
    "</sup></sub>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
